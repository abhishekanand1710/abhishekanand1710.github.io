---
layout: page
title: "Towards Quality Code Generation: Evaluation and Improvement of LLM-Generated Code"
categories:
- projects
tags:
- research
---
How much redundancy is present in a dataset? Does training on the full dataset with low variety in samples lead to overfitting to the data distribution and reduced generalization capabilities? How can we measure the variety present in a dataset before training? The project answers these questions by introducing a variety estimation technique for NLP datasets using pre-trained embeddings and PCA, and demonstrates overfitting to domain data by training on the full dataset leading to poor out-of-distribution performance.
